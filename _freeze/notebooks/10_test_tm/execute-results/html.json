{
  "hash": "2bf4a962517c4250f49c98ef97afaf07",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Empirically verify the perturbation sensitivity of doubly stochastic matrices\n---\n\n```{chatgpt}\n2. Numerical Stability Considerations\n\nNumerical stability refers to how errors—stemming from finite precision arithmetic, rounding, and truncation—propagate through computations involving the matrix.\n\na. Conditioning\n\n\t•\tCondition Number: The condition number of a matrix measures how much the output value can change for a small change in the input. For doubly stochastic matrices, the condition number can vary widely depending on the specific matrix. Some are well-conditioned (small condition numbers), implying that they are relatively stable under numerical perturbations, while others are ill-conditioned (large condition numbers), making them sensitive to errors.\n\t•\tPerturbation Sensitivity: Doubly stochastic matrices can be sensitive to perturbations, especially those near the boundary of the Birkhoff polytope (i.e., those close to permutation matrices). Small numerical errors might push such matrices outside the set of doubly stochastic matrices or significantly alter their properties.\n\n4. Theoretical Insights and Research\n\nResearch into the numerical stability of doubly stochastic matrices highlights several key points:\n\n\t•\tBirkhoff Polytope Structure: The geometric properties of the Birkhoff polytope influence stability. Matrices that lie deep within the polytope (far from permutation matrices) tend to be better conditioned than those near the vertices.\n\t•\tSpectral Properties: The eigenvalues and singular values of doubly stochastic matrices play a role in their numerical behavior. For instance, the presence of eigenvalues close to zero can indicate potential instability.\n\t•\tRandom Doubly Stochastic Matrices: Probabilistic analyses suggest that randomly generated doubly stochastic matrices often exhibit better conditioning on average, though specific instances may still be problematic.\n```\n\n```{julia}\nusing LinearAlgebra\nusing Random\n```\n\n\n\n\n## 2. Generating a Doubly Stochastic Matrix\n\n> There are multiple ways to generate a doubly stochastic matrix. One common method is using the Sinkhorn-Knopp algorithm, which iteratively normalizes the rows and columns of a non-negative matrix to make it doubly stochastic.\n\n\n\n\n```{julia}\n\nfunction sinkhorn_knopp(A; max_iters=1000, tol=1e-9, normalize_rows=true, normalize_cols=true)\n    \"\"\"\n    Applies the Sinkhorn-Knopp algorithm to matrix A to make it doubly stochastic.\n\n    # Arguments\n    - `A::Matrix{Float64}`: Non-negative input matrix\n    - `max_iters::Int`: Maximum number of iterations\n    - `tol::Float64`: Tolerance for convergence\n    - `normalize_rows::Bool`: Whether to normalize rows\n    - `normalize_cols::Bool`: Whether to normalize columns\n\n    # Returns\n    - `Matrix{Float64}`: Doubly stochastic matrix\n    \"\"\"\n    A = copy(A)\n    n, m = size(A)\n    if n != m\n        error(\"Matrix must be square.\")\n    end\n    for iter in 1:max_iters\n        if normalize_rows\n            row_sums = sum(A, dims=2)\n            A = A ./ row_sums\n        end\n        if normalize_cols\n            col_sums = sum(A, dims=1)\n            A = A ./ col_sums\n        end\n        # Check convergence\n        row_diff = normalize_rows ? maximum(abs.(sum(A, dims=2) .- 1)) : 0.0\n        col_diff = normalize_cols ? maximum(abs.(sum(A, dims=1) .- 1)) : 0.0\n        if row_diff < tol && col_diff < tol\n            println(\"Converged in $iter iterations.\")\n            break\n        end\n        if iter == max_iters\n            println(\"Reached maximum iterations without full convergence.\")\n        end\n    end\n    return A\nend\n\n# Example: Generate a random non-negative matrix and make it doubly stochastic\nRandom.seed!(123)  # For reproducibility\nn = 5  # Size of the matrix\nA = rand(n, n)\nA_ds = sinkhorn_knopp(A)\nprintln(\"Doubly Stochastic Matrix A_ds:\\n\")\ndisplay(A_ds)\nprintln(\"Row sums: \", sum(A_ds, dims=2))\nprintln(\"Column sums: \", sum(A_ds, dims=1))\n\ni = 5\nA_ds_i = A_ds^i\nprintln(\"Matrix A_ds^$i:\\n\")\ndisplay(A_ds_i)\nprintln(\"Row sums: \", sum(A_ds_i, dims=2))\nprintln(\"Column sums: \", sum(A_ds_i, dims=1))\n```\n\n\n\n\n## 3. Introducing Perturbations\n\nTo assess perturbation sensitivity, we can introduce small random perturbations to the doubly stochastic matrix and observe how its properties change.\n\n\n\n\n```{julia}\nfunction perturb_matrix(A::Matrix{Float64}, noise_level::Float64)\n    \"\"\"\n    Adds Gaussian noise to matrix A and projects it back to doubly stochastic.\n    \n    # Arguments\n    - `A::Matrix{Float64}`: Original doubly stochastic matrix\n    - `noise_level::Float64`: Standard deviation of Gaussian noise\n    \n    # Returns\n    - `Matrix{Float64}`: Perturbed doubly stochastic matrix\n    \"\"\"\n    n, m = size(A)\n    noise = randn(n, m) * noise_level\n    A_perturbed = A + noise\n    # Ensure non-negativity\n    A_perturbed[A_perturbed .< 0] .= 0.0\n    # Re-apply Sinkhorn-Knopp to make it doubly stochastic\n    A_perturbed_ds = sinkhorn_knopp(A_perturbed)\n    return A_perturbed_ds\nend\n\nfunction perturb_right_stochastic(A::Matrix{Float64}, noise_level::Float64)\n    \"\"\"\n    Perturbs a right stochastic matrix while preserving row sums.\n\n    # Arguments\n    - `A::Matrix{Float64}`: Original right stochastic matrix\n    - `noise_level::Float64`: Standard deviation of Gaussian noise\n\n    # Returns\n    - `Matrix{Float64}`: Perturbed right stochastic matrix\n    \"\"\"\n    n, m = size(A)\n    # Add Gaussian noise\n    noise = randn(n, m) * noise_level\n    A_perturbed = A + noise\n    # Set negative entries to a small positive value to maintain non-negativity\n    A_perturbed[A_perturbed .< 1e-8] .= 1e-8\n    # Renormalize rows to sum to 1\n    row_sums = sum(A_perturbed, dims=2)\n    A_perturbed_rs = A_perturbed ./ row_sums\n    return A_perturbed_rs\nend\n\n\n# Example usage\nnoise_level = 0.05\nA_rs_perturbed = perturb_right_stochastic(A_ds, noise_level)\nprintln(\"Perturbed Right Stochastic Matrix A_rs_perturbed:\\n\", A_rs_perturbed)\nprintln(\"Row sums: \", sum(A_rs_perturbed, dims=2))\nprintln(\"Column sums: \", sum(A_rs_perturbed, dims=1))\n\nA_rs_perturbed_i = A_rs_perturbed^100\nprintln(\"Matrix A_rs_perturbed^$i:\\n\")\ndisplay(A_rs_perturbed_i)\nprintln(\"Row sums: \", sum(A_rs_perturbed_i, dims=2))\nprintln(\"Column sums: \", sum(A_rs_perturbed_i, dims=1)) \n```\n\n```{julia}\n\nfunction compute_metrics(A_original::Matrix{Float64}, A_perturbed::Matrix{Float64})\n    \"\"\"\n    Computes various metrics to assess the difference between two matrices.\n    \n    # Arguments\n    - `A_original::Matrix{Float64}`: Original doubly stochastic matrix\n    - `A_perturbed::Matrix{Float64}`: Perturbed doubly stochastic matrix\n    \n    # Returns\n    - `Dict`: Dictionary containing computed metrics\n    \"\"\"\n    difference = A_original - A_perturbed\n    absolute_diff = maximum(abs.(difference))\n    relative_diff = maximum(abs.(difference) ./ A_original)\n    \n    # Condition numbers\n    cond_original = cond(A_original)\n    cond_perturbed = cond(A_perturbed)\n    \n    # Eigenvalues\n    eigen_original = eigen(A_original).values\n    eigen_perturbed = eigen(A_perturbed).values\n    eigen_diff = maximum(abs.(eigen_original - eigen_perturbed))\n    \n    # Singular values\n    singular_original = svdvals(A_original)\n    singular_perturbed = svdvals(A_perturbed)\n    singular_diff = maximum(abs.(singular_original - singular_perturbed))\n    \n    # Frobenius norm\n    # frob_norm = norm(difference, Frobenius)\n    \n    return Dict(\n        :absolute_difference => absolute_diff,\n        :relative_difference => relative_diff,\n        :condition_number_original => cond_original,\n        :condition_number_perturbed => cond_perturbed,\n        :max_eigenvalue_difference => eigen_diff,\n        :max_singular_difference => singular_diff\n        # :frobenius_norm => frob_norm\n    )\nend\n\n# Compute metrics\nmetrics = compute_metrics(A_ds, A_perturbed)\nprintln(\"Perturbation Metrics:\")\nfor (key, value) in metrics\n    println(\"$key: $value\")\nend\n```\n\n```{julia}\nusing CairoMakie\n# Define a range of noise levels\nnoise_levels = [0.001, 0.005, 0.01, 0.05, 0.1]\n\n# Initialize containers for metrics\nabsolute_diffs = Float64[]\nrelative_diffs = Float64[]\ncond_originals = Float64[]\ncond_perturbed_vals = Float64[]\nmax_eigen_diffs = Float64[]\nmax_singular_diffs = Float64[]\nfrob_norms = Float64[]\n\nfor noise in noise_levels\n    A_p = perturb_matrix(A_ds, noise)\n    m = compute_metrics(A_ds, A_p)\n    push!(absolute_diffs, m[:absolute_difference])\n    push!(relative_diffs, m[:relative_difference])\n    push!(cond_originals, m[:condition_number_original])\n    push!(cond_perturbed_vals, m[:condition_number_perturbed])\n    push!(max_eigen_diffs, m[:max_eigenvalue_difference])\n    push!(max_singular_diffs, m[:max_singular_difference])\n    # push!(frob_norms, m[:frobenius_norm])\nend\n\n# Plotting the results\nf = Figure()\nax = Axis(f[1, 1], xlabel=\"Noise Level\", ylabel=\"Metric Value\", title=\"Sensitivity Analysis\")\n\nplot!(noise_levels, absolute_diffs, label=\"Absolute Difference\", marker=:o)\nplot!(noise_levels, relative_diffs, label=\"Relative Difference\", marker=:o)\naxislegend(ax, position=:lt)\n# plot!(noise_levels, frob_norms, label=\"Frobenius Norm\", marker=:o)\ndisplay(f)\n```\n\n",
    "supporting": [
      "10_test_tm_files"
    ],
    "filters": [],
    "includes": {}
  }
}